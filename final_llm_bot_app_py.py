# -*- coding: utf-8 -*-
"""Final LLM_Bot_App.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FBxyNdbWPZzzU0AOHPWiUAfHRWQYxrBF

##**GPU verification**
"""

!nvidia-smi

"""##**Installing deployment libraries**"""

!pip -q install -U flask pyngrok transformers accelerate peft bitsandbytes sentencepiece

import importlib.metadata as md
print("bitsandbytes:", md.version("bitsandbytes"))

from google.colab import files
files.upload()

!unzip -o propchk_mistral_lora.zip -d /content
!ls -lh /content/propchk_mistral_lora

"""##**Uploading and extracting the fine-tuned LoRA model**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/app.py
# import re
# import torch
# from flask import Flask, request, jsonify
# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
# from peft import PeftModel
# 
# app = Flask(__name__)
# 
# BASE_MODEL = "mistralai/Mistral-7B-Instruct-v0.3"
# LORA_PATH  = "/content/propchk_mistral_lora"
# 
# # -----------------------------
# # Load model (4-bit) + LoRA
# # -----------------------------
# bnb = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.float16,
#     bnb_4bit_use_double_quant=True
# )
# 
# print("ðŸ”¹ Loading tokenizer...")
# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
# tokenizer.pad_token = tokenizer.eos_token
# 
# print("ðŸ”¹ Loading base model (4-bit) on GPU...")
# base = AutoModelForCausalLM.from_pretrained(
#     BASE_MODEL,
#     quantization_config=bnb,
#     device_map="auto"
# )
# base.config.use_cache = False
# 
# print("ðŸ”¹ Loading LoRA adapter...")
# model = PeftModel.from_pretrained(base, LORA_PATH)
# model.eval()
# 
# 
# # -----------------------------
# # Output formatting helper
# # -----------------------------
# SECTIONS = [
#     "Issue type:",
#     "Severity:",
#     "How to inspect:",
#     "Red flags:",
#     "Recommendation:",
#     "Questions to ask builder:"
# ]
# 
# def _cleanup_spaces(s: str) -> str:
#     s = re.sub(r"\s+", " ", s).strip()
#     return s
# 
# def format_response(text: str) -> str:
#     """
#     Converts single-paragraph response into clean multi-line response.
# 
#     Also converts the Questions section into bullets if it contains multiple questions.
#     """
#     if not text:
#         return text
# 
#     # Remove accidental prompt remnants
#     text = text.replace("### Instruction:", "").replace("### Response:", "").strip()
# 
#     # If model already used line breaks, keep them but still clean a bit
#     # We'll parse by finding sections.
#     extracted = {}
# 
#     for i, sec in enumerate(SECTIONS):
#         if sec not in text:
#             continue
# 
#         after = text.split(sec, 1)[1]
#         next_pos = len(after)
# 
#         # find the earliest next section that appears in 'after'
#         for sec2 in SECTIONS[i+1:]:
#             p = after.find(sec2)
#             if p != -1:
#                 next_pos = min(next_pos, p)
# 
#         extracted[sec] = _cleanup_spaces(after[:next_pos])
# 
#     # Fallback: if nothing parsed, just return cleaned text
#     if not extracted:
#         return text
# 
#     # Bulletify questions
#     q_key = "Questions to ask builder:"
#     if q_key in extracted:
#         q_text = extracted[q_key]
# 
#         # Split into questions by '?' if many
#         parts = [p.strip() for p in re.split(r"\?\s*", q_text) if p.strip()]
#         if len(parts) >= 2:
#             bullets = []
#             for p in parts:
#                 # add back '?' unless it already ends with one
#                 if not p.endswith("?"):
#                     p = p + "?"
#                 bullets.append(f"â€¢ {p}")
#             extracted[q_key] = "\n" + "\n".join(bullets)
#         else:
#             extracted[q_key] = q_text
# 
#     # Build final formatted output
#     out_lines = []
#     for sec in SECTIONS:
#         if sec in extracted:
#             value = extracted[sec]
#             if sec == q_key and value.startswith("\n"):
#                 out_lines.append(f"{sec}{value}")
#             else:
#                 out_lines.append(f"{sec} {value}".strip())
# 
#     return "\n\n".join(out_lines)
# 
# 
# # -----------------------------
# # Routes
# # -----------------------------
# @app.route("/health", methods=["GET"])
# def health():
#     return jsonify({"status": "ok"})
# 
# @app.route("/generate", methods=["POST"])
# def generate():
#     data = request.get_json(force=True)
#     user_prompt = (data.get("prompt") or "").strip()
#     if not user_prompt:
#         return jsonify({"error": "Missing prompt"}), 400
# 
#     # Strong formatting instruction to help model
#     prompt = f"""
# ### Instruction:
# {user_prompt}
# 
# Respond strictly in this format:
# Issue type:
# Severity:
# How to inspect:
# Red flags:
# Recommendation:
# Questions to ask builder:
# 
# ### Response:
# """.strip()
# 
#     inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
# 
#     with torch.no_grad():
#         out = model.generate(
#             **inputs,
#             max_new_tokens=int(data.get("max_new_tokens", 220)),
#             do_sample=True,
#             temperature=float(data.get("temperature", 0.4)),
#             top_p=float(data.get("top_p", 0.9)),
#             repetition_penalty=float(data.get("repetition_penalty", 1.15)),
#             eos_token_id=tokenizer.eos_token_id,
#             pad_token_id=tokenizer.eos_token_id
#         )
# 
#     decoded = tokenizer.decode(out[0], skip_special_tokens=True)
# 
#     # keep only response portion
#     if "### Response:" in decoded:
#         decoded = decoded.split("### Response:", 1)[1].strip()
# 
#     cleaned = format_response(decoded)
#     return jsonify({"generated_text": cleaned})
# 
# 
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=5000)
#

"""##**Creating the Flask API (app.py)**

####Starting Flask in background (nohup)
"""

!pkill -f "/content/app.py" || true
!fuser -k 5000/tcp || true
!rm -f /content/flask.log

!nohup python3 /content/app.py > /content/flask.log 2>&1 &
print("âœ… Flask started in background")

"""####Verifying Flask locally"""

import requests
print(requests.get("http://127.0.0.1:5000/health", timeout=5).text)

"""####Creating a public URL using Ngrok"""

from pyngrok import ngrok
ngrok.kill()

ngrok.set_auth_token("37FwvYI1beNr5RbhUtgdqDFFCrX_56QVVXoikHVvUjMDK4sFu")

tunnel = ngrok.connect(5000, "http")
print("âœ… PUBLIC URL:", tunnel.public_url)

"""###**Testing the /generate endpoint (local + public)**"""

import requests
print(requests.get("http://127.0.0.1:5000/health").text)

payload = {"prompt": "Bathroom: leakage under wash basin. What to inspect and how to fix?"}
print(requests.post("http://127.0.0.1:5000/generate", json=payload).json())

import requests

API = "https://lesley-readaptive-dialytically.ngrok-free.dev/generate"

payload = {
    "prompt": "Living Room: Flooring hollowness observed. What does it mean and what should I do?"
}

print(requests.post(API, json=payload, timeout=120).json())

"""##**Streamlit**"""

!pip -q install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app.py
# import streamlit as st
# import requests
# 
# st.set_page_config(page_title="PropChk Inspection Bot", page_icon="ðŸ ", layout="centered")
# 
# with st.sidebar:
#     st.title("ðŸ  PropChk Bot")
#     st.markdown("""
#     **Home Inspection & Handover Assistant**
# 
#     Ask about:
#     - Defects
#     - Inspection checks
#     - Severity
#     - Rectification
#     - Builder questions
#     """)
# 
#     API_BASE = st.text_input("Flask API URL", value="http://127.0.0.1:5000").strip().rstrip("/")
# 
#     st.markdown("---")
#     if st.button("ðŸ§¹ Clear chat"):
#         st.session_state.messages = []
#         st.rerun()
# 
# if "messages" not in st.session_state:
#     st.session_state.messages = [{
#         "role": "assistant",
#         "content": "Hi! ðŸ‘‹ Tell me the **space + issue**.\n\nExample:\n`Bathroom: leakage under wash basin. What to inspect and how to fix?`"
#     }]
# 
# st.title("ðŸ’¬ PropChk Home Inspection Assistant")
# st.caption("Practical inspection guidance for buyers & handover checks")
# 
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])
# 
# def call_api(prompt: str) -> str:
#     url = f"{API_BASE}/generate"
#     payload = {
#         "prompt": prompt,
#         "max_new_tokens": 220,
#         "temperature": 0.4,
#         "top_p": 0.9,
#         "repetition_penalty": 1.15
#     }
#     r = requests.post(url, json=payload, timeout=180)
#     r.raise_for_status()
#     return (r.json().get("generated_text") or "").strip()
# 
# user_prompt = st.chat_input("Type your inspection questionâ€¦")
# 
# if user_prompt:
#     st.session_state.messages.append({"role": "user", "content": user_prompt})
#     with st.chat_message("user"):
#         st.markdown(user_prompt)
# 
#     with st.chat_message("assistant"):
#         with st.spinner("Analyzing inspection issueâ€¦"):
#             try:
#                 answer = call_api(user_prompt)
#                 st.markdown(answer)
#                 st.session_state.messages.append({"role": "assistant", "content": answer})
#             except Exception as e:
#                 st.error(f"API error: {e}")
#                 st.info("Make sure Flask is running and `/health` returns OK.")
#

!pkill -f streamlit || true
!fuser -k 8501/tcp || true
!nohup streamlit run /content/streamlit_app.py --server.port 8501 --server.address 0.0.0.0 > /content/streamlit.log 2>&1 &
print("âœ… Streamlit started in background")

import time
time.sleep(3)
!tail -n 40 /content/streamlit.log

from pyngrok import ngrok
ngrok.kill()

tunnel = ngrok.connect(8501, "http")
print("âœ… STREAMLIT URL:", tunnel.public_url)